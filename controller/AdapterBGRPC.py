# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: AdapterService.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpclib
from betterproto.grpc.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class AdapterReturnCode(betterproto.Enum):
    ADAPTER_RETURN_CODE_UNKNOWN = 0
    """
    Return code for unknown behavior, used whenever any unforseen behavior is
    recognized
    """

    ADAPTER_RETURN_CODE_SUCCESS = 1
    """
    Return code used when the AutoML search process concludes to inform the
    controller this will be the final message
    """

    ADAPTER_RETURN_CODE_PENDING = 2
    """
    Return code used for messages during the AutoML search process is running
    but no status messages are available, informs the controller that more
    messages are coming
    """

    ADAPTER_RETURN_CODE_STATUS_UPDATE = 3
    """
    Return code used for messages during the AutoML search process is running,
    informs the controller that more messages are coming
    """

    ADAPTER_RETURN_CODE_ERROR = 100
    """Return code for error behaviors"""


@dataclass(eq=False, repr=False)
class StartAutoMlRequest(betterproto.Message):
    training_id: str = betterproto.string_field(1)
    """
    Unique training id generated when inserting a new training document into
    MongoDBexample: "63525870394eff35ee175bc2"
    """

    dataset_id: str = betterproto.string_field(2)
    """
    Unique dataset id generated when inserting a new dataset document into
    MongoDBexample: '63515c86b10d04d230dc1482'
    """

    user_id: str = betterproto.string_field(3)
    """
    Unique user id generated by the controller on the create new user request
    from the frontend. Linked to a AspNetUser saved within the MS Sql database
    of the frontendexample: '4a7af128-ac3d-4e3d-89fe-b70f7bfe0c3b'
    """

    dataset_path: str = betterproto.string_field(4)
    """
    Absolute system path where the dataset is saved (single file dataset use
    the file name as ending of the path while multi file dataset will only have
    is root folder of the dataset)example: "app/app-
    data/datasets/....../titanic_train.csv"
    """

    configuration: "StartAutoMlConfiguration" = betterproto.message_field(5)
    """
    StartAutoMlConfiguration object, the specific configuration for the
    training session (see StartAutoMlConfiguration for more detals)example:
    StartAutoMlConfiguration
    """

    dataset_configuration: str = betterproto.string_field(6)
    """
    The current dataset configuration used to open the dataset and column
    schema for tabular datasets as a JSON stringexample:
    '{"file_configuration": {"use_header": true,"start_row": 1,"delimiter":
    "semicolon","escape_character": "\\","decimal_character": ".","encoding":
    "ascii","thousands_seperator": "","datetime_format": "%d/%m/%Y
    %H.%M.%S"},"schema": {"DateTime": {"datatype_detected":
    ":string","datatypes_compatible": [":string", ":categorical",
    ":datetime"],"roles_compatible": [":target", ":ignore",
    ":index"],"datatype_selected": ":datetime","role_selected": ":index"}}'
    """


@dataclass(eq=False, repr=False)
class StartAutoMlConfiguration(betterproto.Message):
    task: str = betterproto.string_field(1)
    """
    The selected ML task set during the wizard configurationexample:
    ":tabular_classification"
    """

    runtime_limit: int = betterproto.int32_field(2)
    """
    The maximum amount of time in minute the training can run before
    cancelation set during the wizard configurationexample: 5
    """

    metric: str = betterproto.string_field(3)
    """
    The ML metric selected during the wizard configurationexample: ":accuracy"
    """

    parameters: Dict[str, "DynamicParameterValue"] = betterproto.map_field(
        4, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    """
    Map of additional parametersexamples:{":use_approach": { "values":
    [":adaboost", ":decision_tree"]},":metric": { "values":
    [":accuracy"]},":some_int": { "values": ["17"]}}
    """


@dataclass(eq=False, repr=False)
class StartAutoMlResponse(betterproto.Message):
    session_id: str = betterproto.string_field(1)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class GetAutoMlStatusRequest(betterproto.Message):
    session_id: str = betterproto.string_field(1)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class GetAutoMlStatusResponse(betterproto.Message):
    return_code: "AdapterReturnCode" = betterproto.enum_field(1)
    """
    The return code describes what type of status message was receivedexample:
    ADAPTER_RETURN_CODE_SUCCESS
    """

    status_update: str = betterproto.string_field(2)
    """
    The status message the AutoML solution wrote on the consoleexample:
    "Starting trial #1"
    """

    path: str = betterproto.string_field(3)
    """
    The absolute path to the model zip containing the model and executable
    python sciptsexample: "app/app-
    data/trainings/USER_ID/DATASET_ID/TRAINING_ID/export/autokeras.zip"
    """

    test_score: str = betterproto.string_field(4)
    """
    Json string of the test scores computed by testing the model with the test
    dataset.example: {":precision": 0.222,":f_measure": 0.9.....}
    """

    prediction_time: float = betterproto.float_field(5)
    """
    The prediction time in milliseconds is the average time the model takes to
    make one predictionexample: 12
    """

    ml_library: List[str] = betterproto.string_field(6)
    """List of ML library used by the found modelexample: [":keras_lib"]"""

    ml_model_type: List[str] = betterproto.string_field(7)
    """
    List of ML model type that the found model is composed offexample:
    [":artificial_neural_network"]
    """

    emission_profile: "CarbonEmission" = betterproto.message_field(8)
    """
    The emission produced by the training of the modelexample: see message
    Carbon Emission
    """


@dataclass(eq=False, repr=False)
class CarbonEmission(betterproto.Message):
    emissions: float = betterproto.double_field(1)
    """
    The emissions produced by the training of this model as kg of
    CO2-eqexample: 0.23232
    """

    emissions_rate: float = betterproto.double_field(2)
    """
    The emissions rate measured in kg of CO2 emitted per MWhexample: 0.23232
    """

    energy_consumed: float = betterproto.double_field(3)
    """Total of consumed energy by all hardware componentsexample: 0.23232"""

    duration: float = betterproto.double_field(4)
    """
    Duration during which emessions are tracked (in our case purely the
    training)example: 0.23232
    """

    cpu_count: int = betterproto.int32_field(5)
    """Count of CPUs used by the processexample: 16"""

    cpu_energy: float = betterproto.double_field(6)
    """Total consumed energy by the CPUsexample: 0.23232"""

    cpu_model: str = betterproto.string_field(7)
    """CPU model nameexample: "AMD Ryzen 7 ......"""

    cpu_power: float = betterproto.double_field(8)
    """Power consumed by the CPUsexample: 27.0"""

    gpu_count: int = betterproto.int32_field(9)
    """Count of GPUs used by the processexample: 1"""

    gpu_energy: float = betterproto.double_field(10)
    """Total consumed energy by the GPUsexample: 0.23232"""

    gpu_model: str = betterproto.string_field(11)
    """GPU model nameexample: "1 x NVIDIA Geforce RTX 3070 ....."""

    gpu_power: float = betterproto.double_field(12)
    """Power consumed by the GPUsexample: 27.0"""

    ram_energy: float = betterproto.double_field(13)
    """Total consumed energy by the RAMexample: 0.23232"""

    ram_power: float = betterproto.double_field(14)
    """Power consumed by the RAMexample: 27.0"""

    ram_total_size: float = betterproto.double_field(15)
    """Total RAM size in GBexample: 15.34"""


@dataclass(eq=False, repr=False)
class CreateExplainerDashboardRequest(betterproto.Message):
    data: str = betterproto.string_field(1)
    """
    The JSON string of data used to perform predictions with the found
    modelexample:{"Cabin": "P3","Sex": "male",....}
    """

    process_json: str = betterproto.string_field(2)
    """
    The process JSON describing the configuration the model will
    useexample:{"training_id": "63525870394eff35ee175bc2","dataset_id":
    "635255a4394eff35ee175bb4","user_id":
    "b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed","dataset_path": "app-data\\datasets\
    \b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed\\635255a4394eff35ee175bb4\\Train.csv"
    ,"configuration": {"task": ":tabular_regression", "target":
    "TARGET(PRICE_IN_LACS)", "runtime_limit": 3, "metric":
    ":accuracy"},"dataset_configuration": "SEE Training schema in WIKI for more
    information https://github.com/hochschule-
    darmstadt/MetaAutoML/wiki/2.-System-Architecture#training-record"}
    """

    session_id: str = betterproto.string_field(3)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class CreateExplainerDashboardResponse(betterproto.Message):
    return_code: "AdapterReturnCode" = betterproto.enum_field(1)
    """
    The return code describes what type of status message was receivedexample:
    ADAPTER_RETURN_CODE_SUCCESS
    """


@dataclass(eq=False, repr=False)
class ExplainModelRequest(betterproto.Message):
    data: str = betterproto.string_field(1)
    """
    The JSON string of data used to perform predictions with the found
    modelexample:{"Cabin": "P3","Sex": "male",....}
    """

    process_json: str = betterproto.string_field(2)
    """
    The process JSON describing the configuration the model will
    useexample:{"training_id": "63525870394eff35ee175bc2","dataset_id":
    "635255a4394eff35ee175bb4","user_id":
    "b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed","dataset_path": "app-data\\datasets\
    \b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed\\635255a4394eff35ee175bb4\\Train.csv"
    ,"configuration": {"task": ":tabular_regression", "target":
    "TARGET(PRICE_IN_LACS)", "runtime_limit": 3, "metric":
    ":accuracy"},"dataset_configuration": "SEE Training schema in WIKI for more
    information https://github.com/hochschule-
    darmstadt/MetaAutoML/wiki/2.-System-Architecture#training-record"}
    """

    session_id: str = betterproto.string_field(3)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class ExplainModelResponse(betterproto.Message):
    probabilities: str = betterproto.string_field(1)
    """
    Json string The probabilies found, using the specific
    modelexample:{"probabilities": [23,32,13,...]}
    """


@dataclass(eq=False, repr=False)
class PredictModelRequest(betterproto.Message):
    process_json: str = betterproto.string_field(1)
    """
    The process JSON describing the configuration the model will
    useexample:{"training_id": "63525870394eff35ee175bc2","dataset_id":
    "635255a4394eff35ee175bb4","user_id":
    "b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed","dataset_path": "app-data\\datasets\
    \b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed\\635255a4394eff35ee175bb4\\Train.csv"
    ,"configuration": {"task": ":tabular_regression", "target":
    "TARGET(PRICE_IN_LACS)", "runtime_limit": 3, "metric":
    ":accuracy"},"dataset_configuration": "SEE Training schema in WIKI for more
    information https://github.com/hochschule-
    darmstadt/MetaAutoML/wiki/2.-System-Architecture#training-record"}
    """


@dataclass(eq=False, repr=False)
class PredictModelResponse(betterproto.Message):
    result_path: str = betterproto.string_field(1)
    """
    The absolute path where the prediction results are persistedexample:
    "app/app-data/datasets/..../predictions/..../autokeras_xxxx.csv"
    """


@dataclass(eq=False, repr=False)
class DynamicParameterValue(betterproto.Message):
    """
    Value type for dynamic training parameters for the auto ml solutions.This
    type is needed, because map<> does not supported "repeated" in the value.
    """

    values: List[str] = betterproto.string_field(1)
    """
    List of values for a parameterexamples:- int: ["17"]- single_value:
    [":accuracy"]- list: [":adaboost", ":decision_tree"]
    """


class AdapterServiceStub(betterproto.ServiceStub):
    async def start_auto_ml(
        self,
        start_auto_ml_request: "StartAutoMlRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "StartAutoMlResponse":
        return await self._unary_unary(
            "/AdapterService/StartAutoMl",
            start_auto_ml_request,
            StartAutoMlResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def get_auto_ml_status(
        self,
        get_auto_ml_status_request: "GetAutoMlStatusRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "GetAutoMlStatusResponse":
        return await self._unary_unary(
            "/AdapterService/GetAutoMlStatus",
            get_auto_ml_status_request,
            GetAutoMlStatusResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def explain_model(
        self,
        explain_model_request: "ExplainModelRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ExplainModelResponse":
        return await self._unary_unary(
            "/AdapterService/ExplainModel",
            explain_model_request,
            ExplainModelResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def create_explainer_dashboard(
        self,
        create_explainer_dashboard_request: "CreateExplainerDashboardRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "CreateExplainerDashboardResponse":
        return await self._unary_unary(
            "/AdapterService/CreateExplainerDashboard",
            create_explainer_dashboard_request,
            CreateExplainerDashboardResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def predict_model(
        self,
        predict_model_request: "PredictModelRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "PredictModelResponse":
        return await self._unary_unary(
            "/AdapterService/PredictModel",
            predict_model_request,
            PredictModelResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )


class AdapterServiceBase(ServiceBase):
    async def start_auto_ml(
        self, start_auto_ml_request: "StartAutoMlRequest"
    ) -> "StartAutoMlResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def get_auto_ml_status(
        self, get_auto_ml_status_request: "GetAutoMlStatusRequest"
    ) -> "GetAutoMlStatusResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def explain_model(
        self, explain_model_request: "ExplainModelRequest"
    ) -> "ExplainModelResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def create_explainer_dashboard(
        self, create_explainer_dashboard_request: "CreateExplainerDashboardRequest"
    ) -> "CreateExplainerDashboardResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def predict_model(
        self, predict_model_request: "PredictModelRequest"
    ) -> "PredictModelResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def __rpc_start_auto_ml(
        self, stream: "grpclib.server.Stream[StartAutoMlRequest, StartAutoMlResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.start_auto_ml(request)
        await stream.send_message(response)

    async def __rpc_get_auto_ml_status(
        self,
        stream: "grpclib.server.Stream[GetAutoMlStatusRequest, GetAutoMlStatusResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.get_auto_ml_status(request)
        await stream.send_message(response)

    async def __rpc_explain_model(
        self, stream: "grpclib.server.Stream[ExplainModelRequest, ExplainModelResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.explain_model(request)
        await stream.send_message(response)

    async def __rpc_create_explainer_dashboard(
        self,
        stream: "grpclib.server.Stream[CreateExplainerDashboardRequest, CreateExplainerDashboardResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.create_explainer_dashboard(request)
        await stream.send_message(response)

    async def __rpc_predict_model(
        self, stream: "grpclib.server.Stream[PredictModelRequest, PredictModelResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.predict_model(request)
        await stream.send_message(response)

    def __mapping__(self) -> Dict[str, grpclib.const.Handler]:
        return {
            "/AdapterService/StartAutoMl": grpclib.const.Handler(
                self.__rpc_start_auto_ml,
                grpclib.const.Cardinality.UNARY_UNARY,
                StartAutoMlRequest,
                StartAutoMlResponse,
            ),
            "/AdapterService/GetAutoMlStatus": grpclib.const.Handler(
                self.__rpc_get_auto_ml_status,
                grpclib.const.Cardinality.UNARY_UNARY,
                GetAutoMlStatusRequest,
                GetAutoMlStatusResponse,
            ),
            "/AdapterService/ExplainModel": grpclib.const.Handler(
                self.__rpc_explain_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                ExplainModelRequest,
                ExplainModelResponse,
            ),
            "/AdapterService/CreateExplainerDashboard": grpclib.const.Handler(
                self.__rpc_create_explainer_dashboard,
                grpclib.const.Cardinality.UNARY_UNARY,
                CreateExplainerDashboardRequest,
                CreateExplainerDashboardResponse,
            ),
            "/AdapterService/PredictModel": grpclib.const.Handler(
                self.__rpc_predict_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                PredictModelRequest,
                PredictModelResponse,
            ),
        }
