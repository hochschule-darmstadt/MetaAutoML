# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: AdapterService.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpclib
from betterproto.grpc.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class AdapterReturnCode(betterproto.Enum):
    ADAPTER_RETURN_CODE_UNKNOWN = 0
    """
    Return code for unknown behavior, used whenever any unforseen behavior is
    recognized
    """

    ADAPTER_RETURN_CODE_SUCCESS = 1
    """
    Return code used when the AutoML search process concludes to inform the
    controller this will be the final message
    """

    ADAPTER_RETURN_CODE_PENDING = 2
    """
    Return code used for messages during the AutoML search process is running
    but no status messages are available, informs the controller that more
    messages are coming
    """

    ADAPTER_RETURN_CODE_STATUS_UPDATE = 3
    """
    Return code used for messages during the AutoML search process is running,
    informs the controller that more messages are coming
    """

    ADAPTER_RETURN_CODE_ERROR = 100
    """Return code for error behaviors"""


@dataclass(eq=False, repr=False)
class StartAutoMlRequest(betterproto.Message):
    training_id: str = betterproto.string_field(1)
    """
    Unique training id generated when inserting a new training document into
    MongoDBexample: "63525870394eff35ee175bc2"
    """

    dataset_id: str = betterproto.string_field(2)
    """
    Unique dataset id generated when inserting a new dataset document into
    MongoDBexample: '63515c86b10d04d230dc1482'
    """

    user_id: str = betterproto.string_field(3)
    """
    Unique user id generated by the controller on the create new user request
    from the frontend. Linked to a AspNetUser saved within the MS Sql database
    of the frontendexample: '4a7af128-ac3d-4e3d-89fe-b70f7bfe0c3b'
    """

    dataset_path: str = betterproto.string_field(4)
    """
    Absolute system path where the dataset is saved (single file dataset use
    the file name as ending of the path while multi file dataset will only have
    is root folder of the dataset)example: "app/app-
    data/datasets/....../titanic_train.csv"
    """

    configuration: "StartAutoMlConfiguration" = betterproto.message_field(5)
    """
    StartAutoMlConfiguration object, the specific configuration for the
    training session (see StartAutoMlConfiguration for more detals)example:
    StartAutoMlConfiguration
    """

    dataset_configuration: str = betterproto.string_field(6)
    """
    The current file configuration used to open the dataset as a JSON
    stringexample: '{"use_header": true,"start_row": 1,"delimiter":
    "comma","escape_character": "\\","decimal_character": "."}'
    """


@dataclass(eq=False, repr=False)
class StartAutoMlConfiguration(betterproto.Message):
    task: str = betterproto.string_field(1)
    """
    The selected ML task set during the wizard configurationexample:
    ":tabular_classification"
    """

    target: str = betterproto.string_field(2)
    """
    The dataset target, if required by ML task set during the wizard
    configurationexample: "Survived"
    """

    runtime_limit: int = betterproto.int32_field(4)
    """
    The maximum amount of time in minute the training can run before
    cancelation set during the wizard configurationexample: 5
    """

    parameters: Dict[str, "DynamicParameterValue"] = betterproto.map_field(
        5, betterproto.TYPE_STRING, betterproto.TYPE_MESSAGE
    )
    """
    Map of additional parametersexamples:{":use_approach": { "values":
    [":adaboost", ":decision_tree"]},":metric": { "values":
    [":accuracy"]},":some_int": { "values": ["17"]}}
    """


@dataclass(eq=False, repr=False)
class StartAutoMlResponse(betterproto.Message):
    session_id: str = betterproto.string_field(1)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class GetAutoMlStatusRequest(betterproto.Message):
    session_id: str = betterproto.string_field(1)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class GetAutoMlStatusResponse(betterproto.Message):
    return_code: "AdapterReturnCode" = betterproto.enum_field(1)
    """
    The return code describes what type of status message was receivedexample:
    ADAPTER_RETURN_CODE_SUCCESS
    """

    status_update: str = betterproto.string_field(2)
    """
    The status message the AutoML solution wrote on the consoleexample:
    "Starting trial #1"
    """

    path: str = betterproto.string_field(3)
    """
    The absolute path to the model zip containing the model and executable
    python sciptsexample: "app/app-
    data/trainings/USER_ID/DATASET_ID/TRAINING_ID/export/autokeras.zip"
    """

    test_score: float = betterproto.float_field(4)
    """
    The test score computed by testing the model with the test datasetexample:
    0.89
    """

    prediction_time: float = betterproto.float_field(5)
    """
    The prediction time in milliseconds is the average time the model takes to
    make one predictionexample: 12
    """

    ml_library: str = betterproto.string_field(6)
    """The ML library used by the found modelexample: ":keras_lib"""

    ml_model_type: str = betterproto.string_field(7)
    """
    The ML model type that the found model is composed offexample:
    ":artificial_neural_network"
    """


@dataclass(eq=False, repr=False)
class ExplainModelRequest(betterproto.Message):
    data: str = betterproto.string_field(1)
    """
    The JSON string of data used to perform predictions with the found
    modelexample:{"Cabin": "P3","Sex": "male",....}
    """

    process_json: str = betterproto.string_field(2)
    """
    The process JSON describing the configuration the model will
    useexample:{"training_id": "63525870394eff35ee175bc2","dataset_id":
    "635255a4394eff35ee175bb4","user_id":
    "b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed","dataset_path": "app-data\\datasets\
    \b84eadcc-765c-43ea-8b7b-
    c0f8d2fbc6ed\\635255a4394eff35ee175bb4\\Train.csv","configuration":
    {"task": ":tabular_regression", "target": "TARGET(PRICE_IN_LACS)",
    "runtime_limit": 3, "metric": ":accuracy"},"dataset_configuration": "SEE
    Training schema in WIKI for more information https://github.com/hochschule-
    darmstadt/MetaAutoML/wiki/2.-System-Architecture#training-record"}
    """

    session_id: str = betterproto.string_field(3)
    """
    Unique session id used to identify a running training session within the
    adapter, generated by the adapter before starting the AutoML solution
    search processexample: "4a8af126-ac3d-4e3d-89fe-b70f7bfe0c3b"
    """


@dataclass(eq=False, repr=False)
class ExplainModelResponse(betterproto.Message):
    probabilities: str = betterproto.string_field(1)
    """
    Json string The probabilies found, using the specific
    modelexample:{"probabilities": [23,32,13,...]}
    """


@dataclass(eq=False, repr=False)
class PredictModelRequest(betterproto.Message):
    process_json: str = betterproto.string_field(1)
    """
    The process JSON describing the configuration the model will
    useexample:{"training_id": "63525870394eff35ee175bc2","dataset_id":
    "635255a4394eff35ee175bb4","user_id":
    "b84eadcc-765c-43ea-8b7b-c0f8d2fbc6ed","dataset_path": "app-data\\datasets\
    \b84eadcc-765c-43ea-8b7b-
    c0f8d2fbc6ed\\635255a4394eff35ee175bb4\\Train.csv","configuration":
    {"task": ":tabular_regression", "target": "TARGET(PRICE_IN_LACS)",
    "runtime_limit": 3, "metric": ":accuracy"},"dataset_configuration": "SEE
    Training schema in WIKI for more information https://github.com/hochschule-
    darmstadt/MetaAutoML/wiki/2.-System-Architecture#training-record"}
    """


@dataclass(eq=False, repr=False)
class PredictModelResponse(betterproto.Message):
    result_path: str = betterproto.string_field(1)
    """
    The absolute path where the prediction results are persistedexample:
    "app/app-data/datasets/..../predictions/..../autokeras_xxxx.csv"
    """


@dataclass(eq=False, repr=False)
class DynamicParameterValue(betterproto.Message):
    """
    Value type for dynamic training parameters for the auto ml solutions.This
    type is needed, because map<> does not supported "repeated" in the value.
    """

    values: List[str] = betterproto.string_field(1)
    """
    List of values for a parameterexamples:- int: ["17"]- single_value:
    [":accuracy"]- list: [":adaboost", ":decision_tree"]
    """


class AdapterServiceStub(betterproto.ServiceStub):
    async def start_auto_ml(
        self,
        start_auto_ml_request: "StartAutoMlRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "StartAutoMlResponse":
        return await self._unary_unary(
            "/AdapterService/StartAutoMl",
            start_auto_ml_request,
            StartAutoMlResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def get_auto_ml_status(
        self,
        get_auto_ml_status_request: "GetAutoMlStatusRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "GetAutoMlStatusResponse":
        return await self._unary_unary(
            "/AdapterService/GetAutoMlStatus",
            get_auto_ml_status_request,
            GetAutoMlStatusResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def explain_model(
        self,
        explain_model_request: "ExplainModelRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ExplainModelResponse":
        return await self._unary_unary(
            "/AdapterService/ExplainModel",
            explain_model_request,
            ExplainModelResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def predict_model(
        self,
        predict_model_request: "PredictModelRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "PredictModelResponse":
        return await self._unary_unary(
            "/AdapterService/PredictModel",
            predict_model_request,
            PredictModelResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )


class AdapterServiceBase(ServiceBase):
    async def start_auto_ml(
        self, start_auto_ml_request: "StartAutoMlRequest"
    ) -> "StartAutoMlResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def get_auto_ml_status(
        self, get_auto_ml_status_request: "GetAutoMlStatusRequest"
    ) -> "GetAutoMlStatusResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def explain_model(
        self, explain_model_request: "ExplainModelRequest"
    ) -> "ExplainModelResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def predict_model(
        self, predict_model_request: "PredictModelRequest"
    ) -> "PredictModelResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def __rpc_start_auto_ml(
        self, stream: "grpclib.server.Stream[StartAutoMlRequest, StartAutoMlResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.start_auto_ml(request)
        await stream.send_message(response)

    async def __rpc_get_auto_ml_status(
        self,
        stream: "grpclib.server.Stream[GetAutoMlStatusRequest, GetAutoMlStatusResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.get_auto_ml_status(request)
        await stream.send_message(response)

    async def __rpc_explain_model(
        self, stream: "grpclib.server.Stream[ExplainModelRequest, ExplainModelResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.explain_model(request)
        await stream.send_message(response)

    async def __rpc_predict_model(
        self, stream: "grpclib.server.Stream[PredictModelRequest, PredictModelResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.predict_model(request)
        await stream.send_message(response)

    def __mapping__(self) -> Dict[str, grpclib.const.Handler]:
        return {
            "/AdapterService/StartAutoMl": grpclib.const.Handler(
                self.__rpc_start_auto_ml,
                grpclib.const.Cardinality.UNARY_UNARY,
                StartAutoMlRequest,
                StartAutoMlResponse,
            ),
            "/AdapterService/GetAutoMlStatus": grpclib.const.Handler(
                self.__rpc_get_auto_ml_status,
                grpclib.const.Cardinality.UNARY_UNARY,
                GetAutoMlStatusRequest,
                GetAutoMlStatusResponse,
            ),
            "/AdapterService/ExplainModel": grpclib.const.Handler(
                self.__rpc_explain_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                ExplainModelRequest,
                ExplainModelResponse,
            ),
            "/AdapterService/PredictModel": grpclib.const.Handler(
                self.__rpc_predict_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                PredictModelRequest,
                PredictModelResponse,
            ),
        }
