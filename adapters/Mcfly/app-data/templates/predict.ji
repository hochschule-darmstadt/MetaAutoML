import os
import sys
import json
import dill
import pandas as pd
import numpy as np
from sktime.datatypes import convert_to
from sktime.datasets import load_from_tsfile
from sktime.datasets import load_from_tsfile_to_dataframe
from sklearn.model_selection import train_test_split
from tensorflow import keras

from enum import Enum, unique


class SplitMethod(Enum):
    SPLIT_METHOD_RANDOM = 0
    SPLIT_METHOD_END = 1


def split_dataset(dataset, json_configuration):
    """
    Split the given dataset into train and test subsets
    """
    split_method = json_configuration["test_configuration"]["method"]
    split_ratio = json_configuration["test_configuration"]["split_ratio"]
    if "random_state" in json_configuration["test_configuration"]:
        random_state = json_configuration["test_configuration"]["random_state"]
    else:
        random_state = 42

    np.random.seed(random_state)

    if int(SplitMethod.SPLIT_METHOD_RANDOM.value) == split_method:
        return train_test_split(
            dataset,
            train_size=split_ratio,
            random_state=random_state,
            shuffle=True,
            stratify=dataset["target"]
        )
    elif split_ratio == 0:
        return None, dataset
    else:
        return train_test_split(
            dataset,
            train_size=split_ratio,
            shuffle=False,
            stratify=dataset["target"]
        )


def read_longitudinal_dataset(file_path, json_configuration):
    """
    Read longitudinal data from the `.ts` file
    """
    dataset = load_from_tsfile_to_dataframe(file_path, return_separate_X_and_y=False)
    dataset = dataset.rename(columns={"class_vals": "target"})
    return split_dataset(dataset, json_configuration)


if __name__ == '__main__':
    file_path = sys.argv[1]
    config_path = sys.argv[2]
    sys_path_0 = sys.path[0]

    with open(config_path) as file:
        config_json = json.load(file)

    # split training set
    df_train, df_test = read_longitudinal_dataset(file_path, config_json)
    X_test, y_test = df_test.drop(["target"], axis=1), df_test["target"]
    y_test = y_test.to_numpy()

    print("Loading the one_hot_encoder")
    with open(os.path.join(sys_path_0, 'one_hot_encoder_mcfly.p'), 'rb') as file:
        one_hot_encoder = dill.load(file)

    # Convert longitudinal data to numpy3D
    X_test = convert_to(X_test, to_type="numpy3D")
    X_test = np.swapaxes(X_test, 1, 2)

    # Load the saved model
    model_path = os.path.join(sys_path_0, 'model_mcfly.p')
    print('model_path:', model_path)

    print('Loading mcfly model')
    loaded_model = keras.models.load_model(model_path)

    # Predict values
    print('predicting ...')
    predicted_y = loaded_model.predict(X_test)
    predicted_y = one_hot_encoder.inverse_transform(predicted_y).reshape(1, -1)[0]
    print('y_predicted:', predicted_y)
    print('y_true:', y_test)

    # Save the predictions
    print('saving predictions ...')
    df_predictions = pd.DataFrame(data=predicted_y, columns=["predicted"])
    df_predictions.to_csv(sys_path_0 + "/predictions.csv", index=False)
    print('Saved predictions.csv')
