{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset and make ydata-profiling dashboard\n",
    "\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv(\"../data/datasetRuntimeData.csv\")\n",
    "#Drop index col\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "#Remove special symbole\n",
    "df['AutoML_adapter'] = df['AutoML_adapter'].str.replace(':', '')\n",
    "#Apply one hot encoding\n",
    "df = pd.get_dummies(df, columns=['AutoML_adapter'], prefix='', prefix_sep='')\n",
    "\n",
    "#profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "#profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 17:27:08,806 INFO: [AutoFeat] The 2 step feature engineering process could generate up to 5565 features.\n",
      "2025-01-04 17:27:08,807 INFO: [AutoFeat] With 256 data points this new feature matrix would use about 0.01 gb of space.\n",
      "2025-01-04 17:27:08,810 INFO: [feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]               0/             15 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 17:27:12,340 INFO: [feateng] Generated 21 transformed features from 15 original features - done.\n",
      "2025-01-04 17:27:12,344 INFO: [feateng] Step 2: first combination of features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]             300/            630 feature tuples combined\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\Desktop\\MetaAutoML\\utils\\analysis\\ai_based_optimization\\predicting_optimal_runtime\\.venv2\\Lib\\site-packages\\numpy\\core\\_methods.py:187: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n",
      "c:\\Users\\alex\\Desktop\\MetaAutoML\\utils\\analysis\\ai_based_optimization\\predicting_optimal_runtime\\.venv2\\Lib\\site-packages\\numpy\\core\\_methods.py:176: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n",
      "2025-01-04 17:27:13,414 INFO: [feateng] Generated 592 feature combinations from 630 original feature tuples - done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]             600/            630 feature tuples combined\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 17:27:13,417 INFO: [feateng] Generated altogether 646 new features in 2 steps\n",
      "2025-01-04 17:27:13,418 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n",
      "2025-01-04 17:27:13,451 INFO: [feateng] Generated a total of 364 additional features\n",
      "2025-01-04 17:27:13,456 INFO: [featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data...done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 17:27:16,091 INFO: [featsel] Feature selection run 2/5\n",
      "2025-01-04 17:27:18,529 INFO: [featsel] Feature selection run 3/5\n",
      "2025-01-04 17:27:20,766 INFO: [featsel] Feature selection run 4/5\n",
      "2025-01-04 17:27:22,804 INFO: [featsel] Feature selection run 5/5\n",
      "2025-01-04 17:27:25,372 INFO: [featsel] 14 features after 5 feature selection runs\n",
      "c:\\Users\\alex\\Desktop\\MetaAutoML\\utils\\analysis\\ai_based_optimization\\predicting_optimal_runtime\\.venv2\\Lib\\site-packages\\autofeat\\featsel.py:270: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  if np.max(np.abs(correlations[c].ravel()[:i])) < 0.9:\n",
      "2025-01-04 17:27:25,376 INFO: [featsel] 11 features after correlation filtering\n",
      "2025-01-04 17:27:25,407 INFO: [featsel] 6 features after noise filtering\n",
      "2025-01-04 17:27:25,408 INFO: [AutoFeat] Computing 5 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     4/    5 new features\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 17:27:27,038 INFO: [AutoFeat]     5/    5 new features ...done.\n",
      "2025-01-04 17:27:27,040 INFO: [AutoFeat] Final dataframe with 20 feature columns (5 new).\n",
      "2025-01-04 17:27:27,041 INFO: [AutoFeat] Training final regression model.\n",
      "2025-01-04 17:27:27,054 INFO: [AutoFeat] Trained model: largest coefficients:\n",
      "2025-01-04 17:27:27,056 INFO: 88.72974285286506\n",
      "2025-01-04 17:27:27,056 INFO: 28.174471 * autogluon*log(dataset_size_in_mb)\n",
      "2025-01-04 17:27:27,057 INFO: 16.500999 * gama*log(dataset_cols)\n",
      "2025-01-04 17:27:27,057 INFO: 1.278435 * sqrt(duplicated_rows)*gama\n",
      "2025-01-04 17:27:27,058 INFO: 0.000562 * dataset_cols**2*h2o_automl\n",
      "2025-01-04 17:27:27,061 INFO: [AutoFeat] Final score: 0.1709\n",
      "2025-01-04 17:27:27,065 INFO: [AutoFeat] Computing 5 new features.\n",
      "2025-01-04 17:27:27,075 INFO: [AutoFeat]     5/    5 new features ...done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     4/    5 new features\r"
     ]
    }
   ],
   "source": [
    "#AutoFeat for all automls\n",
    "\n",
    "\n",
    "X = df.drop([\"runtime_limit\"], axis=1)\n",
    "y = df[\"runtime_limit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "### get list of selected features ###\n",
    "afreg = AutoFeatRegressor(verbose=1)\n",
    "\n",
    "X_train = afreg.fit_transform(X_train, y_train)\n",
    "X_test = afreg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create prediction plot\n",
    "# def create_prediction_plot(y_test, predictions):\n",
    "#     prediction_results = pd.DataFrame({\n",
    "#         'runtime_limit_is': y_test,\n",
    "#         'runtime_limit_predicted': predictions\n",
    "#     })\n",
    "\n",
    "#     best_case_x = [0, 5, 10, 20, 40, 80, 160, 320, 640]\n",
    "#     best_case_y = [0, 5, 10, 20, 40, 80, 160, 320, 640]\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.scatterplot(\n",
    "#         x='runtime_limit_predicted', \n",
    "#         y='runtime_limit_is', \n",
    "#         data=prediction_results, \n",
    "#         color='gray', marker='o'  # Using a distinct color palette\n",
    "#     )\n",
    "\n",
    "#     plt.plot(best_case_x, best_case_y)\n",
    "#     plt.xscale('log', base=10)  # Logarithmic scale for x-axis\n",
    "#     plt.yscale('log', base=10)  # Logarithmic scale for y-axis\n",
    "\n",
    "\n",
    "#     # Find the limits in log space\n",
    "#     x_min, x_max = 1, 100\n",
    "#     y_min, y_max = 1, np.exp(6.6)\n",
    "\n",
    "#     # Determine the limits to make them symmetrical in log space\n",
    "#     log_min = min(np.log10(x_min), np.log10(y_min))\n",
    "#     log_max = max(np.log10(x_max), np.log10(y_max))\n",
    "\n",
    "#     # Apply the symmetrical limits\n",
    "#     plt.xlim([10**log_min, 10**log_max])\n",
    "#     plt.ylim([10**log_min, 10**log_max])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     plt.xlabel('Optimal runtime predicted')\n",
    "#     plt.ylabel('Optimal runtime measured')\n",
    "#     #plt.legend(title='AutoML Solution', bbox_to_anchor=(1.05, 0.5), loc='center left')\n",
    "#     #plt.title('Actual vs Predicted Runtime Limits')\n",
    "#     plt.grid(True)\n",
    "#     #plt.legend(title='Series')\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = MAE(predictions, y_test)\n",
    "\n",
    "    print(f\"{type(model)} Mean Absolute Error (MAE):\", round(mae))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_lgbm_model(model,):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "\n",
    "    # Define parameters for the LightGBM model\n",
    "    params = {\n",
    "        'objective': 'regression',  # Set the objective as regression\n",
    "        'metric': 'mae',            # Use mean absolute error as the evaluation metric\n",
    "        'verbose': 1                # Disable verbose output\n",
    "    }\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    num_round = 100\n",
    "    model = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = MAE(predictions, y_test)\n",
    "\n",
    "    print(f\"{type(model)} Mean Absolute Error (MAE):\", round(mae))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.dummy.DummyRegressor'> Mean Absolute Error (MAE): 104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 228\n",
      "[LightGBM] [Info] Number of data points in the train set: 256, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 105.683594\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "<class 'lightgbm.basic.Booster'> Mean Absolute Error (MAE): 113\n",
      "<class 'sklearn.linear_model._base.LinearRegression'> Mean Absolute Error (MAE): 328\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> Mean Absolute Error (MAE): 104\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'> Mean Absolute Error (MAE): 8648\n",
      "<class 'sklearn.linear_model._ridge.Ridge'> Mean Absolute Error (MAE): 320\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'> Mean Absolute Error (MAE): 316\n",
      "<class 'sklearn.linear_model._coordinate_descent.ElasticNet'> Mean Absolute Error (MAE): 296\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> Mean Absolute Error (MAE): 107\n",
      "<class 'sklearn.linear_model._bayes.BayesianRidge'> Mean Absolute Error (MAE): 307\n",
      "<class 'sklearn.svm._classes.SVR'> Mean Absolute Error (MAE): 104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = { \"Baseline\": DummyRegressor(strategy=\"median\"), \n",
    "          \"LightGBM\": None, \n",
    "          \"Linear Regression\": LinearRegression(), \n",
    "          \"Decision Tree\": DecisionTreeRegressor(random_state=42), \n",
    "          \"Sklearn Neural Network\": MLPRegressor(random_state=42), \n",
    "          \"Ridge\": Ridge(), \n",
    "          \"Lasso\": Lasso(), \n",
    "          \"Elastic\": ElasticNet(), \n",
    "          \"Random Forest\": RandomForestRegressor(), \n",
    "          \"Bayesian\": BayesianRidge(), \n",
    "          \"SVM\": SVR()}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name == \"LightGBM\":\n",
    "        models[model_name] = train_lgbm_model(model)\n",
    "    else:\n",
    "        models[model_name] = train_model(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
