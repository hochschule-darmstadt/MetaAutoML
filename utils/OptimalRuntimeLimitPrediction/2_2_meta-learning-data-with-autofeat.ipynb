{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset and make ydata-profiling dashboard\n",
    "\n",
    "#from ydata_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv(\"./data/datasetData.csv\")\n",
    "#Drop index col\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "#Remove special symbole\n",
    "df['AutoML_solution'] = df['AutoML_solution'].str.replace(':', '')\n",
    "#Apply one hot encoding\n",
    "df = pd.get_dummies(df, columns=['AutoML_solution'], prefix='', prefix_sep='')\n",
    "\n",
    "#profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "#profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 08:26:45,971 INFO: [AutoFeat] The 2 step feature engineering process could generate up to 5565 features.\n",
      "2024-06-20 08:26:45,972 INFO: [AutoFeat] With 256 data points this new feature matrix would use about 0.01 gb of space.\n",
      "2024-06-20 08:26:45,973 INFO: [feateng] Step 1: transformation of original features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]               0/             15 features transformed\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 08:26:50,312 INFO: [feateng] Generated 21 transformed features from 15 original features - done.\n",
      "2024-06-20 08:26:50,315 INFO: [feateng] Step 2: first combination of features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng]             400/            630 feature tuples combined\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\Desktop\\Meta-Learning\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:199: RuntimeWarning: overflow encountered in reduce\n",
      "c:\\Users\\alex\\Desktop\\Meta-Learning\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:188: RuntimeWarning: overflow encountered in multiply\n",
      "2024-06-20 08:26:51,071 INFO: [feateng] Generated 592 feature combinations from 630 original feature tuples - done.\n",
      "2024-06-20 08:26:51,073 INFO: [feateng] Generated altogether 646 new features in 2 steps\n",
      "2024-06-20 08:26:51,074 INFO: [feateng] Removing correlated features, as well as additions at the highest level\n",
      "2024-06-20 08:26:51,088 INFO: [feateng] Generated a total of 364 additional features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Scaling data.../            630 feature tuples combined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 08:26:57,402 INFO: [featsel] Feature selection run 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 08:27:02,097 INFO: [featsel] Feature selection run 2/5\n",
      "2024-06-20 08:27:03,903 INFO: [featsel] Feature selection run 3/5\n",
      "2024-06-20 08:27:05,985 INFO: [featsel] Feature selection run 4/5\n",
      "2024-06-20 08:27:08,174 INFO: [featsel] Feature selection run 5/5\n",
      "2024-06-20 08:27:10,445 INFO: [featsel] 15 features after 5 feature selection runs\n",
      "c:\\Users\\alex\\Desktop\\Meta-Learning\\.venv\\Lib\\site-packages\\autofeat\\featsel.py:270: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  if np.max(np.abs(correlations[c].ravel()[:i])) < 0.9:\n",
      "2024-06-20 08:27:10,449 INFO: [featsel] 11 features after correlation filtering\n",
      "2024-06-20 08:27:10,478 INFO: [featsel] 7 features after noise filtering\n",
      "2024-06-20 08:27:10,479 INFO: [AutoFeat] Computing 6 new features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     5/    6 new features\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 08:27:12,631 INFO: [AutoFeat]     6/    6 new features ...done.\n",
      "2024-06-20 08:27:12,632 INFO: [AutoFeat] Final dataframe with 21 feature columns (6 new).\n",
      "2024-06-20 08:27:12,633 INFO: [AutoFeat] Training final regression model.\n",
      "2024-06-20 08:27:12,646 INFO: [AutoFeat] Trained model: largest coefficients:\n",
      "2024-06-20 08:27:12,646 INFO: 96.524318759094\n",
      "2024-06-20 08:27:12,646 INFO: 2.805375 * autogluon*sqrt(duplicated_cols)\n",
      "2024-06-20 08:27:12,647 INFO: 1.260376 * gama*log(dataset_cols)\n",
      "2024-06-20 08:27:12,647 INFO: 0.000552 * dataset_cols**2*h2o_automl\n",
      "2024-06-20 08:27:12,649 INFO: [AutoFeat] Final score: 0.1233\n",
      "2024-06-20 08:27:12,652 INFO: [AutoFeat] Computing 6 new features.\n",
      "2024-06-20 08:27:12,662 INFO: [AutoFeat]     6/    6 new features ...done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoFeat]     5/    6 new features\r"
     ]
    }
   ],
   "source": [
    "#AutoFeat for all automls\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "X = df.drop([\"runtime_limit\"], axis=1)\n",
    "y = df[\"runtime_limit\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "### get list of selected features ###\n",
    "afreg = AutoFeatRegressor(verbose=1)\n",
    "\n",
    "X_train = afreg.fit_transform(X_train, y_train)\n",
    "X_test = afreg.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create prediction plot\n",
    "def create_prediction_plot(y_test, predictions):\n",
    "    prediction_results = pd.DataFrame({\n",
    "        'runtime_limit_is': y_test,\n",
    "        'runtime_limit_predicted': predictions\n",
    "    })\n",
    "\n",
    "    best_case_x = [0, 5, 10, 20, 40, 80, 160, 320, 640]\n",
    "    best_case_y = [0, 5, 10, 20, 40, 80, 160, 320, 640]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x='runtime_limit_predicted', \n",
    "        y='runtime_limit_is', \n",
    "        data=prediction_results, \n",
    "        color='gray', marker='o'  # Using a distinct color palette\n",
    "    )\n",
    "\n",
    "    plt.plot(best_case_x, best_case_y)\n",
    "    plt.xscale('log', base=10)  # Logarithmic scale for x-axis\n",
    "    plt.yscale('log', base=10)  # Logarithmic scale for y-axis\n",
    "\n",
    "\n",
    "    # Find the limits in log space\n",
    "    x_min, x_max = 1, 100\n",
    "    y_min, y_max = 1, np.exp(6.6)\n",
    "\n",
    "    # Determine the limits to make them symmetrical in log space\n",
    "    log_min = min(np.log10(x_min), np.log10(y_min))\n",
    "    log_max = max(np.log10(x_max), np.log10(y_max))\n",
    "\n",
    "    # Apply the symmetrical limits\n",
    "    plt.xlim([10**log_min, 10**log_max])\n",
    "    plt.ylim([10**log_min, 10**log_max])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel('Optimal runtime predicted')\n",
    "    plt.ylabel('Optimal runtime measured')\n",
    "    #plt.legend(title='AutoML Solution', bbox_to_anchor=(1.05, 0.5), loc='center left')\n",
    "    #plt.title('Actual vs Predicted Runtime Limits')\n",
    "    plt.grid(True)\n",
    "    #plt.legend(title='Series')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = MAE(predictions, y_test)\n",
    "\n",
    "    print(f\"{type(model)} Mean Absolute Error (MAE):\", round(mae))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "def train_lgbm_model(model,):\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "\n",
    "    # Define parameters for the LightGBM model\n",
    "    params = {\n",
    "        'objective': 'regression',  # Set the objective as regression\n",
    "        'metric': 'mae',            # Use mean absolute error as the evaluation metric\n",
    "        'verbose': 1                # Disable verbose output\n",
    "    }\n",
    "\n",
    "    # Train the LightGBM model\n",
    "    num_round = 100\n",
    "    model = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    \n",
    "    # Compute MAE\n",
    "    mae = MAE(predictions, y_test)\n",
    "\n",
    "    print(f\"{type(model)} Mean Absolute Error (MAE):\", round(mae))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 228\n",
      "[LightGBM] [Info] Number of data points in the train set: 256, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 105.742188\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "<class 'lightgbm.basic.Booster'> Mean Absolute Error (MAE): 112\n",
      "<class 'sklearn.dummy.DummyRegressor'> Mean Absolute Error (MAE): 104\n",
      "<class 'sklearn.linear_model._base.LinearRegression'> Mean Absolute Error (MAE): 337\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> Mean Absolute Error (MAE): 104\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'> Mean Absolute Error (MAE): 2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex\\Desktop\\Meta-Learning\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=1.74572e-21): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._ridge.Ridge'> Mean Absolute Error (MAE): 328\n",
      "<class 'sklearn.linear_model._coordinate_descent.Lasso'> Mean Absolute Error (MAE): 325\n",
      "<class 'sklearn.linear_model._coordinate_descent.ElasticNet'> Mean Absolute Error (MAE): 309\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> Mean Absolute Error (MAE): 104\n",
      "<class 'sklearn.linear_model._bayes.BayesianRidge'> Mean Absolute Error (MAE): 306\n",
      "<class 'sklearn.svm._classes.SVR'> Mean Absolute Error (MAE): 104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = { \"LightGBM\": None, \"Baseline\": DummyRegressor(strategy=\"median\"), \"Linear Regression\": LinearRegression(), \n",
    "          \"Decision Tree\": DecisionTreeRegressor(random_state=42), \n",
    "          \"Sklearn Neural Network\": MLPRegressor(random_state=42), \"Ridge\": Ridge(), \n",
    "          \"Lasso\": Lasso(), \"Elastic\": ElasticNet(), \"Random Forest\": RandomForestRegressor(), \n",
    "          \"Bayesian\": BayesianRidge(), \"SVM\": SVR()}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if model_name == \"LightGBM\":\n",
    "        models[model_name] = train_lgbm_model(model)\n",
    "    else:\n",
    "        models[model_name] = train_model(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
