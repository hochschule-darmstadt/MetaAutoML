{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bson.objectid import ObjectId\n",
    "import os\n",
    "import collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_automl_rows(df: pd.DataFrame, auto_ml_solution: str, dataset_size_mb: float, \n",
    "                       dataset_rows: int, dataset_cols: int, dataset_missing_values:float, dataset_duplicated_row_values:float, dataset_duplicated_col_values:float, dataset_outlier_row_values:float, runtime_limit : int):\n",
    "    new_row = {\"AutoML_solution\":auto_ml_solution, \"dataset_size_in_mb\" : dataset_size_mb, \"dataset_rows\": dataset_rows, \"dataset_cols\": dataset_cols, \"missing_values\": dataset_missing_values, \"duplicated_rows\": dataset_duplicated_row_values, \"duplicated_cols\": dataset_duplicated_col_values, \"outliers\": dataset_outlier_row_values,\n",
    "               \"runtime_limit\": runtime_limit}\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True )\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_dataset_meta_informations(data):\n",
    "        dataset_size_byte = data[\"analysis\"][\"size_bytes\"]\n",
    "        dataset_size_mb = dataset_size_byte / 1000  / 1000\n",
    "        dataset_rows = data[\"analysis\"][\"number_of_rows\"]\n",
    "        dataset_cols = data[\"analysis\"][\"number_of_columns\"]\n",
    "        #compute ration of missing values\n",
    "        dataset_missing_values_total = 0\n",
    "        for k, v in data[\"analysis\"][\"missings_per_column\"].items():\n",
    "            if isinstance(v, dict):\n",
    "                for k1, v1 in v.items():\n",
    "                    if isinstance(v1, dict):\n",
    "                        for k2, v2 in v1.items():\n",
    "                            dataset_missing_values_total = dataset_missing_values_total + v2\n",
    "                    else:\n",
    "                        dataset_missing_values_total = dataset_missing_values_total + v1\n",
    "            else:\n",
    "                dataset_missing_values_total = dataset_missing_values_total + v\n",
    "        # dataset_missing_values = round(dataset_missing_values_total / (dataset_rows + dataset_cols), 2)\n",
    "        dataset_missing_values = dataset_missing_values_total\n",
    "        #compute ration of duplicated rows\n",
    "        # dataset_duplicated_row_values_total = 0\n",
    "        # for v in data[\"analysis\"][\"duplicate_rows\"]:\n",
    "        #     dataset_duplicated_row_values_total = dataset_duplicated_row_values_total + len(v)\n",
    "        # dataset_duplicated_row_values = round(dataset_duplicated_row_values_total / dataset_rows, 2)\n",
    "        dataset_duplicated_row_values = len(data[\"analysis\"][\"duplicate_rows\"])\n",
    "        #compute ration of duplicated col\n",
    "        # dataset_duplicated_col_values_total = 0\n",
    "        # for v in data[\"analysis\"][\"duplicate_columns\"]:\n",
    "        #     dataset_duplicated_col_values_total = dataset_duplicated_col_values_total + len(v)\n",
    "        # dataset_duplicated_col_values = round(dataset_duplicated_col_values_total / dataset_cols, 2)\n",
    "        dataset_duplicated_col_values = len(data[\"analysis\"][\"duplicate_columns\"])\n",
    "        #compute ration of outlier rows\n",
    "        # dataset_outlier_row_values_total = []\n",
    "        # for k, v in data[\"analysis\"][\"outlier\"].items():\n",
    "        #     dataset_outlier_row_values_total += v\n",
    "        # dataset_outlier_row_values = round(len(set(dataset_outlier_row_values_total)) / dataset_rows, 2)\n",
    "        dataset_outlier_row_values_total = 0\n",
    "        for k, v in data[\"analysis\"][\"outlier\"].items():\n",
    "            dataset_outlier_row_values_total += len(v)\n",
    "        dataset_outlier_row_values = dataset_outlier_row_values_total\n",
    "        \n",
    "        return dataset_size_mb, dataset_missing_values, dataset_duplicated_row_values, dataset_duplicated_col_values, dataset_outlier_row_values, dataset_rows, dataset_cols\n",
    "\n",
    "def generate_automl_runtime_dataset(trainings: collection ,datasets: collection, models: collection,file_path: str):\n",
    "    metric = \":balanced_accuracy\"\n",
    "    header_row = [\"AutoML_solution\", \"dataset_name\", \"dataset_size_in_mb\", \"dataset_rows\", \"dataset_cols\", \"missing_values\", \"duplicated_rows\", \"duplicated_cols\", \"outliers\",\n",
    "                   \"runtime_limit\", metric]\n",
    "    df = pd.DataFrame(columns = header_row)\n",
    "    result_dict = {}\n",
    "    failed_value = 0\n",
    "    for dataset in datasets.find():\n",
    "            #Only use dataset with training series\n",
    "            if dataset[\"lifecycle_state\"] == \"active\" and dataset[\"name\"] in [\"airlines\", \"albert\", \"KDDCup09_appetency\", \"electricity\", \"bank-marketing\", \\\n",
    "                                                                        \"Amazon_employee_access\", \"riccardo\", \"eeg-eye-state\", \"jm1\", \"SpeedDating\", \\\n",
    "                                                                        \"mushroom\", \"christine\", \"phoneme\", \"Bioresponse\", \"kr-vs-kp\", \\\n",
    "                                                                        \"kc1\", \"pc4\", \"profb\", \"credit-approval\", \"breast-w\", \\\n",
    "                                                                        \n",
    "                                                                        \"covertype\", \"dionis\", \"Devnagari-Script\", \"jannis\", \"Fashion-MNIST\", \\\n",
    "                                                                        \"shuttle\", \"tamilnadu-electricity\", \"letter\", \"gas-drift\", \"har\", \\\n",
    "                                                                        \"artificial-characters\", \"optdigits\", \"waveform-5000\", \"splice\", \"car\", \\\n",
    "                                                                        \"one-hundred-plants-margin\", \"vehicle\", \"eucalyptus\", \"soybean\", \"LED-display-domain-7digit\" ]:\n",
    "                automl_dict = {}\n",
    "                #Find all trainings from the training series\n",
    "                for training in trainings.find({\"dataset_id\": str(dataset[\"_id\"])}):\n",
    "                    #Get result scores for all series\n",
    "                    for model_id in training[\"model_ids\"]:\n",
    "                        for data in models.find({\"_id\": ObjectId(model_id)}):\n",
    "                            if data[\"lifecycle_state\"] == \"active\":\n",
    "                                if data[\"auto_ml_solution\"] in [\":autogluon\", \":evalml\", \":flaml\", \":gama\", \":lama\", \":h2o_automl\", \":pycaret\", \":tpot\"]:\n",
    "\n",
    "                                    if data[\"auto_ml_solution\"] not in automl_dict:\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]] = {}\n",
    "                                    if data[\"status\"] == \"failed\":\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]][training[\"configuration\"][\"runtime_limit\"]] = failed_value\n",
    "                                    else:\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]][training[\"configuration\"][\"runtime_limit\"]] = data[\"test_score\"][metric]\n",
    "                    \n",
    "                def runtimes(dict):\n",
    "                    return [key for key in [5, 10, 20, 40, 80, 160, 320, 640] if key not in dict]\n",
    "                \n",
    "                missing_runtime = runtimes(automl_dict[\":gama\"])\n",
    "                if len(missing_runtime) != 0:\n",
    "                    print(f\"MISSING RUNTIMES FOR DATASET {dataset['name']}, {missing_runtime}\")\n",
    "                best_scores = {}\n",
    "                #Find the best score X runtime for each AutoML\n",
    "                for automl, runtime_scores in automl_dict.items():\n",
    "                    best_score = None\n",
    "                    best_runtime = None\n",
    "                    \n",
    "                    # Iterate over runtimes starting from the lowest\n",
    "                    for runtime, score in sorted(runtime_scores.items()):\n",
    "                        if best_score is None or score >= best_score + 0.001:  # Check for 0.1% improvement\n",
    "                            best_score = score\n",
    "                            best_runtime = runtime\n",
    "                    \n",
    "                    best_scores[automl] = (best_runtime, best_score)\n",
    "\n",
    "                #Meta Informations\n",
    "                for automl, runtime_scores in best_scores.items():\n",
    "                    dataset_size_mb, dataset_missing_values, dataset_duplicated_row_values, dataset_duplicated_col_values, dataset_outlier_row_values, dataset_rows, dataset_cols = get_dataset_meta_informations(dataset)\n",
    "                    df = insert_automl_rows(df, automl, dataset[\"name\"], dataset_size_mb, dataset_rows, dataset_cols, dataset_missing_values, dataset_duplicated_row_values, dataset_duplicated_col_values, dataset_outlier_row_values, runtime_scores[0])\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    df.to_csv(os.path.join(file_path,\"datasetData.csv\"))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_automl_runtime_dataset(trainings: collection ,datasets: collection, models: collection,file_path: str):\n",
    "    metric = \":balanced_accuracy\"\n",
    "    header_row = [\"AutoML_solution\", \"dataset_size_in_mb\", \"dataset_rows\", \"dataset_cols\", \"missing_values\", \"duplicated_rows\", \"duplicated_cols\", \"outliers\",\n",
    "                   \"runtime_limit\"]\n",
    "    df = pd.DataFrame(columns = header_row)\n",
    "    result_dict = {}\n",
    "    failed_value = 0\n",
    "    for dataset in datasets.find():\n",
    "            #Only use dataset with training series\n",
    "            if dataset[\"lifecycle_state\"] == \"active\" and dataset[\"name\"] in [\"airlines\", \"albert\", \"KDDCup09_appetency\", \"electricity\", \"bank-marketing\", \\\n",
    "                                                                        \"Amazon_employee_access\", \"riccardo\", \"eeg-eye-state\", \"jm1\", \"SpeedDating\", \\\n",
    "                                                                        \"mushroom\", \"christine\", \"phoneme\", \"Bioresponse\", \"kr-vs-kp\", \\\n",
    "                                                                        \"kc1\", \"pc4\", \"profb\", \"credit-approval\", \"breast-w\", \\\n",
    "                                                                        \n",
    "                                                                        \"covertype\", \"dionis\", \"Devnagari-Script\", \"jannis\", \"Fashion-MNIST\", \\\n",
    "                                                                        \"shuttle\", \"tamilnadu-electricity\", \"letter\", \"gas-drift\", \"har\", \\\n",
    "                                                                        \"artificial-characters\", \"optdigits\", \"waveform-5000\", \"splice\", \"car\", \\\n",
    "                                                                        \"one-hundred-plants-margin\", \"vehicle\", \"eucalyptus\", \"soybean\", \"LED-display-domain-7digit\"]:\n",
    "                automl_dict = {}\n",
    "                #Find all trainings from the training series\n",
    "                for training in trainings.find({\"dataset_id\": str(dataset[\"_id\"])}):\n",
    "                    #Get result scores for all series\n",
    "                    for model_id in training[\"model_ids\"]:\n",
    "                        for data in models.find({\"_id\": ObjectId(model_id)}):\n",
    "                            if data[\"lifecycle_state\"] == \"active\":\n",
    "                                if data[\"auto_ml_solution\"] in [\":autogluon\", \":evalml\", \":flaml\", \":gama\", \":lama\", \":h2o_automl\", \":pycaret\", \":tpot\"]:\n",
    "\n",
    "                                    if data[\"auto_ml_solution\"] not in automl_dict:\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]] = {}\n",
    "                                    if data[\"status\"] == \"failed\":\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]][training[\"configuration\"][\"runtime_limit\"]] = failed_value\n",
    "                                    else:\n",
    "                                        automl_dict[data[\"auto_ml_solution\"]][training[\"configuration\"][\"runtime_limit\"]] = data[\"test_score\"][metric]\n",
    "                    \n",
    "                def runtimes(dict):\n",
    "                    return [key for key in [5, 10, 20, 40, 80, 160, 320, 640] if key not in dict]\n",
    "                \n",
    "                missing_runtime = runtimes(automl_dict[\":gama\"])\n",
    "                if len(missing_runtime) != 0:\n",
    "                    print(f\"MISSING RUNTIMES FOR DATASET {dataset['name']}, {missing_runtime}\")\n",
    "                best_scores = {}\n",
    "                #Find the best score X runtime for each AutoML\n",
    "                for automl, runtime_scores in automl_dict.items():\n",
    "                    best_score = None\n",
    "                    best_runtime = None\n",
    "                    \n",
    "                    # Iterate over runtimes starting from the lowest\n",
    "                    for runtime, score in sorted(runtime_scores.items()):\n",
    "                        if best_score is None or score >= best_score + 0.001:  # Check for 0.1% improvement\n",
    "                            best_score = score\n",
    "                            best_runtime = runtime\n",
    "                    \n",
    "                    best_scores[automl] = (best_runtime, best_score)\n",
    "\n",
    "                #Meta Informations\n",
    "                for automl, runtime_scores in best_scores.items():\n",
    "                    dataset_size_mb, dataset_missing_values, dataset_duplicated_row_values, dataset_duplicated_col_values, dataset_outlier_row_values, dataset_rows, dataset_cols = get_dataset_meta_informations(dataset)\n",
    "                    df = insert_automl_rows(df, automl, dataset_size_mb, dataset_rows, dataset_cols, dataset_missing_values, dataset_duplicated_row_values, dataset_duplicated_col_values, dataset_outlier_row_values, runtime_scores[0])\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        os.makedirs(file_path)\n",
    "\n",
    "    df.to_csv(os.path.join(file_path,\"datasetData.csv\"))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_21704\\763424416.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script is for updating the runtime prediction parameters\n",
    "\n",
    "For calculating the new runtime prediction parameters edit the following variables or rename the collections in MongoDBCompass\n",
    "\"\"\"\n",
    "# Set here your database connection\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:5050/\")\n",
    "\n",
    "# fill in the name of your database\n",
    "db = client[\"744e63b1-56f6-4fa8-bae4-be31ff8ee100\"]\n",
    "\n",
    "# Collection Name\n",
    "#ändere die Namen demenstrpchend nach den collection namen aus deiner Datenbank ab\n",
    "trainings = db[\"trainings\"]\n",
    "datasets = db[\"datasets\"]\n",
    "models = db[\"models\"]\n",
    "\n",
    "# Only edit when changing the folder structure\n",
    "file_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "\n",
    "#reads the information from the database and saves them in a csv file\n",
    "generate_automl_runtime_dataset(trainings, datasets, models, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
