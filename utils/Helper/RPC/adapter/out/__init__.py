# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: AdapterService.proto
# plugin: python-betterproto
from dataclasses import dataclass
from typing import (
    TYPE_CHECKING,
    AsyncIterator,
    Dict,
    List,
    Optional,
)

import betterproto
import grpclib
from betterproto.grpc.grpclib_server import ServiceBase


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class AdapterReturnCode(betterproto.Enum):
    ADAPTER_RETURN_CODE_UNKNOWN = 0
    ADAPTER_RETURN_CODE_SUCCESS = 1
    ADAPTER_RETURN_CODE_STATUS_UPDATE = 2
    ADAPTER_RETURN_CODE_ERROR = 100


@dataclass(eq=False, repr=False)
class TestAdapterRequest(betterproto.Message):
    test_data: str = betterproto.string_field(1)
    process_json: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class TestAdapterResponse(betterproto.Message):
    predictions: List[str] = betterproto.string_field(1)
    score: float = betterproto.float_field(2)
    predictiontime: float = betterproto.float_field(3)


@dataclass(eq=False, repr=False)
class StartAutoMlRequest(betterproto.Message):
    process_json: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class StartAutoMlResponse(betterproto.Message):
    return_code: "AdapterReturnCode" = betterproto.enum_field(1)
    status_update: str = betterproto.string_field(2)
    output_json: str = betterproto.string_field(3)
    test_score: float = betterproto.float_field(4)
    validation_score: float = betterproto.float_field(5)
    runtime: int = betterproto.int32_field(6)
    predictiontime: float = betterproto.float_field(7)
    library: str = betterproto.string_field(8)
    model: str = betterproto.string_field(9)


@dataclass(eq=False, repr=False)
class ExplainModelRequest(betterproto.Message):
    data: str = betterproto.string_field(1)
    process_json: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class ExplainModelResponse(betterproto.Message):
    probabilities: str = betterproto.string_field(1)


class AdapterServiceStub(betterproto.ServiceStub):
    async def start_auto_ml(
        self,
        start_auto_ml_request: "StartAutoMlRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> AsyncIterator["StartAutoMlResponse"]:
        async for response in self._unary_stream(
            "/AdapterService/StartAutoML",
            start_auto_ml_request,
            StartAutoMlResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        ):
            yield response

    async def test_adapter(
        self,
        test_adapter_request: "TestAdapterRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "TestAdapterResponse":
        return await self._unary_unary(
            "/AdapterService/TestAdapter",
            test_adapter_request,
            TestAdapterResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def explain_model(
        self,
        explain_model_request: "ExplainModelRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ExplainModelResponse":
        return await self._unary_unary(
            "/AdapterService/ExplainModel",
            explain_model_request,
            ExplainModelResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )


class AdapterServiceBase(ServiceBase):
    async def start_auto_ml(self, start_auto_ml_request: "StartAutoMlRequest") -> AsyncIterator["StartAutoMlResponse"]:
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def test_adapter(
        self, test_adapter_request: "TestAdapterRequest"
    ) -> "TestAdapterResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def explain_model(
        self, explain_model_request: "ExplainModelRequest"
    ) -> "ExplainModelResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def __rpc_start_auto_ml(
        self, stream: "grpclib.server.Stream[StartAutoMlRequest, StartAutoMlResponse]"
    ) -> None:
        request = await stream.recv_message()
        await self._call_rpc_handler_server_stream(
            self.start_auto_ml,
            stream,
            request,
        )

    async def __rpc_test_adapter(
        self, stream: "grpclib.server.Stream[TestAdapterRequest, TestAdapterResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.test_adapter(request)
        await stream.send_message(response)

    async def __rpc_explain_model(
        self, stream: "grpclib.server.Stream[ExplainModelRequest, ExplainModelResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.explain_model(request)
        await stream.send_message(response)

    def __mapping__(self) -> Dict[str, grpclib.const.Handler]:
        return {
            "/AdapterService/StartAutoML": grpclib.const.Handler(
                self.__rpc_start_auto_ml,
                grpclib.const.Cardinality.UNARY_STREAM,
                StartAutoMlRequest,
                StartAutoMlResponse,
            ),
            "/AdapterService/TestAdapter": grpclib.const.Handler(
                self.__rpc_test_adapter,
                grpclib.const.Cardinality.UNARY_UNARY,
                TestAdapterRequest,
                TestAdapterResponse,
            ),
            "/AdapterService/ExplainModel": grpclib.const.Handler(
                self.__rpc_explain_model,
                grpclib.const.Cardinality.UNARY_UNARY,
                ExplainModelRequest,
                ExplainModelResponse,
            ),
        }
